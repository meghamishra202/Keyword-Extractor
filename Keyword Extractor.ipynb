{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa6d5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9241e27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde5cdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c664a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0791a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd= os.getcwd()\n",
    "\n",
    "df= pd.read_csv(cwd+\"\\\\archive\\\\papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72c9201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f79d2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract',\n",
       "       'paper_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a002b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two',\n",
       " 'these',\n",
       " 'hadn',\n",
       " 'against',\n",
       " 'down',\n",
       " 's',\n",
       " 'how',\n",
       " 'both',\n",
       " 'show',\n",
       " 'until',\n",
       " 'if',\n",
       " 'between',\n",
       " 'all',\n",
       " 'figure',\n",
       " \"wouldn't\",\n",
       " 'of',\n",
       " 'large',\n",
       " 'isn',\n",
       " 'up',\n",
       " 're',\n",
       " 'over',\n",
       " 'he',\n",
       " 'fig',\n",
       " 'we',\n",
       " 'and',\n",
       " 'don',\n",
       " 'three',\n",
       " 'what',\n",
       " 'themselves',\n",
       " 'our',\n",
       " 'while',\n",
       " \"don't\",\n",
       " 'who',\n",
       " \"aren't\",\n",
       " 'him',\n",
       " 'needn',\n",
       " 'but',\n",
       " 'into',\n",
       " \"should've\",\n",
       " \"she's\",\n",
       " 'not',\n",
       " 'y',\n",
       " 'shouldn',\n",
       " 'were',\n",
       " 'd',\n",
       " 'during',\n",
       " 'am',\n",
       " 'this',\n",
       " 'why',\n",
       " \"that'll\",\n",
       " \"won't\",\n",
       " 'ours',\n",
       " 'yours',\n",
       " \"you're\",\n",
       " 'be',\n",
       " 'a',\n",
       " 'by',\n",
       " 'below',\n",
       " 'also',\n",
       " 'at',\n",
       " 'that',\n",
       " 'there',\n",
       " 'because',\n",
       " 'they',\n",
       " 'wasn',\n",
       " 'haven',\n",
       " \"mightn't\",\n",
       " \"mustn't\",\n",
       " 'herself',\n",
       " 'ain',\n",
       " 'result',\n",
       " 'an',\n",
       " 'do',\n",
       " 'with',\n",
       " 'will',\n",
       " 'being',\n",
       " 'your',\n",
       " 'same',\n",
       " 'yourself',\n",
       " \"isn't\",\n",
       " \"you've\",\n",
       " 'or',\n",
       " 'own',\n",
       " 'mightn',\n",
       " 'through',\n",
       " 'wouldn',\n",
       " 'itself',\n",
       " 'for',\n",
       " \"shouldn't\",\n",
       " 'other',\n",
       " 'whom',\n",
       " 'mustn',\n",
       " 'off',\n",
       " 'theirs',\n",
       " 'more',\n",
       " 'won',\n",
       " 'where',\n",
       " \"doesn't\",\n",
       " \"you'll\",\n",
       " \"haven't\",\n",
       " 'four',\n",
       " 'has',\n",
       " 'himself',\n",
       " 'are',\n",
       " 'above',\n",
       " 'couldn',\n",
       " 'should',\n",
       " 'which',\n",
       " 'using',\n",
       " 'eight',\n",
       " 'shan',\n",
       " 'hasn',\n",
       " 'very',\n",
       " \"didn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " \"wasn't\",\n",
       " 'only',\n",
       " 'having',\n",
       " \"hadn't\",\n",
       " \"hasn't\",\n",
       " 'image',\n",
       " 'as',\n",
       " 'further',\n",
       " 'some',\n",
       " 'can',\n",
       " \"needn't\",\n",
       " 'now',\n",
       " \"couldn't\",\n",
       " 'm',\n",
       " 't',\n",
       " 'my',\n",
       " 'had',\n",
       " 'any',\n",
       " \"weren't\",\n",
       " 'too',\n",
       " 'aren',\n",
       " 'his',\n",
       " 'again',\n",
       " 'doing',\n",
       " 'seven',\n",
       " 'she',\n",
       " 'myself',\n",
       " 'such',\n",
       " 'to',\n",
       " 'each',\n",
       " 'few',\n",
       " 'their',\n",
       " \"shan't\",\n",
       " 'was',\n",
       " 'didn',\n",
       " 'under',\n",
       " 'i',\n",
       " 'most',\n",
       " 'o',\n",
       " 'about',\n",
       " 'its',\n",
       " 'after',\n",
       " 'you',\n",
       " 'the',\n",
       " 'them',\n",
       " 'those',\n",
       " 'have',\n",
       " 'nor',\n",
       " 'ma',\n",
       " 'weren',\n",
       " 'sample',\n",
       " 'than',\n",
       " 'then',\n",
       " 'been',\n",
       " 'on',\n",
       " 'did',\n",
       " 'one',\n",
       " 'before',\n",
       " 'does',\n",
       " 'is',\n",
       " 'yourselves',\n",
       " 'out',\n",
       " 'no',\n",
       " \"you'd\",\n",
       " 'so',\n",
       " 'll',\n",
       " 'her',\n",
       " 'when',\n",
       " 'just',\n",
       " 'nine',\n",
       " 'five',\n",
       " 'doesn',\n",
       " 'once',\n",
       " 'hers',\n",
       " 'in',\n",
       " 'me',\n",
       " 'ourselves',\n",
       " 've',\n",
       " 'from',\n",
       " 'here']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words= set(stopwords.words('english'))\n",
    "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
    "             \"show\", \"result\", \"large\", \n",
    "             \"also\", \"one\", \"two\", \"three\", \n",
    "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
    "stop_words= list(stop_words.union(new_words))\n",
    "stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "087c96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    text= text.lower()\n",
    "    # remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    #convert to list from string\n",
    "    text= text.split()\n",
    "    \n",
    "    #remove stop words\n",
    "    text= [word for word in text if word not in stop_words]\n",
    "    \n",
    "    # removing words having less than 3 letters\n",
    "    text=[word for word in text if len(word)>=3]\n",
    "    \n",
    "    # lemmatize\n",
    "    lmtzr= WordNetLemmatizer()\n",
    "    text= [lmtzr.lemmatize(word) for word in text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= df['paper_text'].apply(lambda x:pre_process(x))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de1e9c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    767 self-organization associative database app...\n",
       "1    683 mean field theory layer visual cortex appl...\n",
       "2    394 storing covariance associative long?term p...\n",
       "3    bayesian query construction neural network mod...\n",
       "4    neural network ensembles, cross validation, ac...\n",
       "Name: paper_text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1226b6",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d31f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vocabulary of words\n",
    "cv= CountVectorizer(max_df= 0.95, max_features= 10000, ngram_range= (1,3))\n",
    "word_count_vector= cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bea00688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515da0c6",
   "metadata": {},
   "source": [
    "### Function for keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b621e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5faec374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Megha\\anaconda3\\envs\\Projects\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f546f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(idx, docs):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f36acbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d6c6f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Algorithms for Non-negative Matrix Factorization\n",
      "\n",
      "=====Abstract=====\n",
      "Non-negative matrix factorization (NMF) has previously been shown to \r\n",
      "be a useful decomposition for multivariate data. Two different multi- \r\n",
      "plicative algorithms for NMF are analyzed. They differ only slightly in \r\n",
      "the multiplicative factor used in the update rules. One algorithm can be \r\n",
      "shown to minimize the conventional least squares error while the other \r\n",
      "minimizes the generalized Kullback-Leibler divergence. The monotonic \r\n",
      "convergence of both algorithms can be proven using an auxiliary func- \r\n",
      "tion analogous to that used for proving convergence of the Expectation- \r\n",
      "Maximization algorithm. The algorithms can also be interpreted as diag- \r\n",
      "onally rescaled gradient descent, where the rescaling factor is optimally \r\n",
      "chosen to ensure convergence. \n",
      "\n",
      "===Keywords===\n",
      "ht 0.654\n",
      "update 0.224\n",
      "update rule 0.216\n",
      "auxiliary 0.165\n",
      "non negative matrix 0.164\n",
      "negative matrix 0.163\n",
      "nmf 0.143\n",
      "multiplicative 0.137\n",
      "matrix 0.128\n",
      "rule 0.123\n"
     ]
    }
   ],
   "source": [
    "idx=941\n",
    "keywords=get_keywords(idx, docs)\n",
    "print_results(idx,keywords, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875e2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
